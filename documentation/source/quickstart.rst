Quickstart
==========

Here are some of the most commonly used classes and methods in the library.


Kernel herding
--------------
Kernel herding is one (greedy) approach to coreset construction.
A Kernel herding solver can be created by supplying a :class:`~coreax.kernel.Kernel`
object, such as a :class:`~coreax.kernel.SquaredExponentialKernel`. A coreset is then
generated by calling the :meth:`~coreax.solvers.KernelHerding.reduce` method on the
original dataset.

Note that, throughout the codebase, there are block versions of herding for fitting
within memory constraints. These methods partition the data into blocks before carrying
out the coreset algorithm, restricting the maximum size of variables handled in the
process.

.. literalinclude:: snippets/kernel_herding.py


Kernel herding with weighting
-----------------------------
A coreset can be weighted, a so-called **weighted coreset**, to attribute importance to
each point and to better approximate the underlying data distribution. Optimal weights
can be determined by implementing a :class:`~coreax.weights.WeightsOptimiser`, such as
the :class:`~coreax.weights.MMDWeightsOptimiser` weights optimiser.

.. literalinclude:: snippets/kernel_herding_with_weighting.py


Kernel herding with refine
--------------------------
To improve the quality of a coreset, a **refine** step can be executed. These functions
work by substituting points from the coreset with points from the original dataset such
that some metric decreases. This improves the coreset quality because the refined coreset
better captures the underlying distribution of the original data, as measured by the
reduced metric.

There are several different approaches to refining a coreset, which can be found in the
children and methods of :meth:`~coreax.solvers.RefinementSolver`. In the example below,
we instantiate a :class:`~coreax.solvers.RefinementSolver`, specifically a
:class:`coreax.solvers.KernelHerding` solver, and then call the
:meth:`~coreax.solvers.RefinementSolver.refine` method on the solution yielded in the
prior call to :meth:`~coreax.solvers.Solver.reduce`.

.. literalinclude:: snippets/kernel_herding_with_refine.py


Scalable herding
----------------
For large :math:`n` or :math:`d`, you may run into time or memory issues. The class
:class:`~coreax.solvers.composite.MapReduce` uses partitioning to tractably compute an
approximate coreset in reasonable time. There is a necessary impact on coreset quality,
for a dramatic improvement in computation time. These methods can be used by simply
composing a :class:`~coreax.solvers.Solver` in the previous examples with
:class:`~coreax.solvers.composite.MapReduce` and setting the parameter ``leaf_size`` in
line with memory requirements.

.. literalinclude:: snippets/scalable_herding.py


For large :math:`d`, it is usually worth reducing dimensionality using PCA. See
:mod:`examples.pounce_map_reduce` for an example.

Stein kernel herding
--------------------
We have implemented a version of kernel herding that uses a **Stein kernel**, which
targets kernelised Stein discrepancy (KSD) :cite:`liu2016kernelized` rather than MMD.
This can often give better integration error in practice, but it can be slower than
using a simpler kernel targeting MMD. To use Stein kernel herding, we have to define a
continuous approximation to the discrete measure, e.g. using kernel density estimation
(KDE), or an estimate the score function :math:`\nabla \log f_X(\mathbf{x})` of a
continuous PDF from a finite set of samples. In this example, we use a Stein kernel with
a squared exponential base kernel, computing the score function explicitly.

.. literalinclude:: snippets/stein_kernel_herding.py


Score matching example
----------------------
The score function, :math:`\nabla \log f_X(\mathbf{x})`, of a distribution is the
derivative of the log-density function. This function is required when evaluating Stein
kernels. However, it can be difficult to specify analytically in practice.

To resolve this, we have implemented an approximation of the score function using a
neural network :cite:`ssm`. This approximate score function can then be passed directly
to a Stein kernel, removing any requirement for analytical derivation. More details on
score matching methods implemented are found in :mod:`coreax.score_matching`.

.. literalinclude:: snippets/score_matching.py


JIT compilation
---------------
JAX enables us to perform just-in-time (JIT) compilation of our code (providing certain
mild conditions are met), with the potential to significantly improve runtime
performance. It is for this reason that in all the examples, we always call
:code:`eqx.filter_jit(...)` on the reduction method. Because we already have equinox as
a dependency, it makes sense to use this nicer variant of the JIT transformation, over
:func:`jax.jit`.
`See here <https://docs.kidger.site/equinox/api/transformations/#equinox.filter_jit>`
for more information.

We can apply the JIT transformation to most methods within Coreax, but we do not apply
these transformation by default. This is because we typically want to apply the JIT
transformation at the highest possible level/scope to give the compiler the best chance
of performing all the possible optimisations, at the cost of high compile times.
